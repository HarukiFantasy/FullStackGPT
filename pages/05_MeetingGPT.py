import streamlit as st
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

st.title("Sitemap URL Parser")

sitemap_url = st.text_input("Enter Sitemap URL", "https://developers.cloudflare.com/sitemap-0.xml")
keyword = st.text_input("Enter keyword to filter URLs (optional)", "")

def fetch_dynamic_content(url):
    """ JavaScriptê°€ ë Œë”ë§ëœ í›„ì˜ HTMLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    driver.get(url)
    driver.implicitly_wait(7)  # ğŸš€ JS ì‹¤í–‰ í›„ ë¡œë”© ì‹œê°„ ì¦ê°€
    html = driver.page_source
    driver.quit()
    
    return html

def parse_page(soup):
    """ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ """
    content_tags = ["main", "article", "section", "div"]
    for tag in content_tags:
        content = soup.find(tag)
        if content:
            return content.get_text().strip()

    return soup.get_text().strip()

def parse_urls(sitemap_url, keyword):
    if sitemap_url:
        try:
            # ì‚¬ì´íŠ¸ë§µ URLì—ì„œ XML ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
            response = requests.get(sitemap_url)
            response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬

            # XML íŒŒì‹±
            root = ET.fromstring(response.content)
            # ê¸°ë³¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì§€ì •
            ns = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}
            urls = []

            # ëª¨ë“  <url> ìš”ì†Œ ìˆœíšŒí•˜ë©° <loc> íƒœê·¸ì˜ í…ìŠ¤íŠ¸(ì‹¤ì œ URL) ì¶”ì¶œ
            for url_elem in root.findall("ns:url", ns):
                loc = url_elem.find("ns:loc", ns)
                if loc is not None:
                    url_text = loc.text
                    # í‚¤ì›Œë“œê°€ ì…ë ¥ëœ ê²½ìš°, URLì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if keyword:
                        if keyword.lower() in url_text.lower():
                            urls.append(url_text)
                    else:
                        urls.append(url_text)

            return urls  # âœ… URL ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        except Exception as e:
            st.error(f"Error fetching/parsing sitemap: {e}")
            return []  # âœ… ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# âœ… ë¦¬í„´ê°’ì„ ë°›ì•„ì„œ Streamlit UIì— í‘œì‹œ
filtered_urls = parse_urls(sitemap_url, keyword)

# âœ… ë°˜í™˜ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì¶œë ¥
if filtered_urls:
    st.write("Filtered URLs:" if keyword else "Extracted URLs:")
    st.write(filtered_urls)
    for page_url in filtered_urls: 
        raw_html = fetch_dynamic_content(page_url)
        soup = BeautifulSoup(raw_html, "html.parser")
        st.write(parse_page(soup))
else:
    st.write("No URLs found with the given keyword." if keyword else "No URLs found.")
